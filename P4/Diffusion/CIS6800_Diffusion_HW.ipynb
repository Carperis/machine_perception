{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyqfM87ha-av"
      },
      "source": [
        "# CIS 6800 Project: VAE and Diffusion Transformer\n",
        "\n",
        "Instructions:\n",
        "\n",
        "*  In this HW you will implement a patch-based VAE and train a Diffusion model using the pre-trained VAE checkpoint.\n",
        "*  We provided you with a zipfile that contains a subset of 10,000 images from the CelebA dataset.\n",
        "*  The Diffusion training session lasts a couple hours, so you should start part (b) as early as you can!\n",
        "*  **Please submit your ipynb notebook as well as a pdf version of it. Include all visualizations and answers to the questions.**\n",
        "*   Part A is due Wed 10/28, and Part B is due Wed 11/4.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Diffusion Transformer Paper: [https://arxiv.org/pdf/2212.09748](https://arxiv.org/pdf/2212.09748)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGOFTvUDfZPg",
        "outputId": "5a5080cb-611c-401b-c6fd-7c34172fdefc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.5/869.5 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install -q torch torchvision numpy tqdm datasets torch-ema pytorch-lightning timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jk8k1y5JfGTM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms as tf\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torch_ema import ExponentialMovingAverage as EMA\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange, repeat  # You can learn about einops at https://einops.rocks\n",
        "from itertools import pairwise\n",
        "from accelerate import Accelerator\n",
        "from types import SimpleNamespace\n",
        "from typing import Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "\n",
        "import zipfile\n",
        "import io\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BfMhHvCsft4r"
      },
      "outputs": [],
      "source": [
        "# Set the seed for reproducibility\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj1sSnbJpMpj"
      },
      "source": [
        "# Part A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS6ZZ5oKU6mO"
      },
      "source": [
        "# Variational Autoencoder (VAE)\n",
        "\n",
        "\n",
        "In this part of the project you will train a Patch-based VAE to capture key facial features on on a subset of the CelebA dataset, consisting of 10000 face images. You can use the code below to load images from the given zipfile. We recommend resizing the images to (32, 32) for the following tasks but you're welcome to experiment with different sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXd8vlpSgFA0",
        "outputId": "aa405b79-3eff-4a47-d900-0f9034dda8e2"
      },
      "outputs": [],
      "source": [
        "class CelebADataset(Dataset):\n",
        "    def __init__(self, zip_file, transform=None):\n",
        "        self.zip_file = zip_file\n",
        "        self.transform = transform\n",
        "\n",
        "        # Open the zip file and get the list of images\n",
        "        self.zip = zipfile.ZipFile(self.zip_file, 'r')\n",
        "        self.image_list = [file for file in self.zip.namelist() if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image name from the list\n",
        "        img_name = self.image_list[idx]\n",
        "        try:\n",
        "            # Read image data from the zip file\n",
        "            with self.zip.open(img_name) as img_file:\n",
        "                img_data = img_file.read()\n",
        "                img = Image.open(io.BytesIO(img_data)).convert('RGB')\n",
        "\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "        except zipfile.BadZipFile:\n",
        "            # print(f\"BadZipFile error encountered with image {img_name}. Skipping this file.\")\n",
        "            return None\n",
        "\n",
        "        return img\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Filter out None values (e.g., images that couldn't be loaded)\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    # If the batch is empty after filtering, return None (can be skipped by DataLoader)\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "# Usage of the dataset and dataloader\n",
        "def get_celeba_dataloader(zip_path, batch_size=32, image_size=(32, 32)):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    dataset = CelebADataset(zip_path, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "# Function to display original and reconstructed images (4 images only)\n",
        "def show_original_reconstructed(orig_images, recon_images, epoch):\n",
        "    # Move the images back to CPU and denormalize\n",
        "    orig_images = orig_images.cpu().numpy()\n",
        "    recon_images = recon_images.cpu().numpy()\n",
        "\n",
        "    # Clip the values to the valid range [0, 1] for display\n",
        "    orig_images = np.clip(orig_images * 0.5 + 0.5, 0, 1)  # Denormalize and clip\n",
        "    recon_images = np.clip(recon_images * 0.5 + 0.5, 0, 1)  # Denormalize and clip\n",
        "\n",
        "    # Plot images side by side (4 images)\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(10, 4))\n",
        "    for i in range(4):\n",
        "        # Original image\n",
        "        axes[0, i].imshow(orig_images[i].transpose(1, 2, 0))  # Correct shape for imshow\n",
        "        axes[0, i].axis('off')\n",
        "        axes[0, i].set_title(\"Original\")\n",
        "\n",
        "        # Reconstructed image\n",
        "        axes[1, i].imshow(recon_images[i].transpose(1, 2, 0))  # Correct shape for imshow\n",
        "        axes[1, i].axis('off')\n",
        "        axes[1, i].set_title(\"Reconstructed\")\n",
        "\n",
        "    plt.suptitle(f'Epoch {epoch}: Original vs Reconstructed')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7tEj3hebptH_"
      },
      "outputs": [],
      "source": [
        "#Initialize your dataloaders here\n",
        "zip_file_path = \"celeba_10000_images.zip\"\n",
        "dataloader = get_celeba_dataloader(zip_file_path, batch_size=128, image_size=(32, 32))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GiyNPlynmVe"
      },
      "source": [
        "### Patch-based Variational Autoencoder\n",
        "\n",
        "PatchVAE divides an input image into smaller patches and operates on these patches rather than the entire image at once. By processing images in patches, the PatchVAE learns to capture fine-grained (local) and high-level (global) patterns, depending on the patch size. In this section you should:\n",
        "\n",
        "1.   Implement the PatchEmbed Class. This class is designed to create overlapping patches from an image, embed those patches into a latent space, and provide a method to reconstruct the image from the latent representations.\n",
        "\n",
        "2.   Implement the PatchVAE model.\n",
        "\n",
        "    *   The `encode` method convert patch embeddings into latent variables (mu and logvar) through a convolutional encoder. Rearrange patches to treat them as channels before passing them through the encoder.\n",
        "    *   The `reparameterize` method applied the reparameterization trick `z = mu + eps * std` where `eps` is sampled from a standard normal distribution.\n",
        "    *  The `decode` method convert the latent variable z back into patch embeddings, then reconstruct the original image from these patches\n",
        "    *  The `forward` method first patchifies the image, encode, reparameterize, decode, and finally reconstruct the image\n",
        "    *  The `compute_loss` calculates the reconstruction loss (Mean Squared Error) and the KL divergence loss to encourage the latent space to follow a normal distribution. Combine these losses to get the total VAE loss.\n",
        "    *  The `sample` method generates new random images from the learned latent space. Random latent vector `z` are sampled, decoded into patches, and reconstructed into full images.\n",
        "\n",
        "\n",
        "3.   Train the model on the faces dataset. **Experiment with different patch sizes and report the patch size with the best tradeoff between low-level details and high-level texture. Plot the training and validation loss and visualize a couple reconstructed images during training.**\n",
        "\n",
        "4. After training, generate sample images from the latent space. Specifically, you need to sample latent vectors from a normal distribution, decode the latent varaibles, and reconstruct the images. **Visualize 4 generated examples. Do the generated images resemble realistic human faces? Explain in a couple sentences.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KFJY8FNIghTb"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    # This class is designed to create overlapping patches from an image, embed those \n",
        "    # patches into a latent space, and provide a method to reconstruct the image from the latent representations.\n",
        "    def __init__(self, img_size=128, patch_size=32, stride=8, channels=3, embed_dim=128, bias=True):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Use Conv2D to create image patches of size (patch_size, patch_size) with overlapping regions.\n",
        "\n",
        "        Each patch should have embedding size embed_dim.\n",
        "        \"\"\"\n",
        "        self.patch_size = patch_size\n",
        "        self.stride = stride\n",
        "        self.embed_dim = embed_dim\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Conv2d to generate overlapping patches (from image to latent space)\n",
        "        self.proj = nn.Conv2d(channels, embed_dim, kernel_size=patch_size, stride=stride, bias=bias)\n",
        "\n",
        "        # Transposed Conv2d to reconstruct patches from latent space to RGB (from latent to image space)\n",
        "        self.deconv = nn.ConvTranspose2d(embed_dim, 3, kernel_size=patch_size, stride=stride, bias=bias)\n",
        "\n",
        "        H_out = (img_size - self.patch_size) // self.stride + 1 \n",
        "        W_out = (img_size - self.patch_size) // self.stride + 1 \n",
        "        self.num_patches = H_out * W_out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input x is an image of size [B, C, img_size, img_size] [50, 3, 32, 32]\n",
        "\n",
        "        Return patches of size [B, num_patches, embed_dim] [B, 16, 64]\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        patches = self.proj(x)\n",
        "        patches = patches.view(x.size(0), self.num_patches, self.embed_dim)\n",
        "        ######## END TODO ########\n",
        "        return patches # [, 16, 64]\n",
        "\n",
        "    def reconstruct(self, patches, img_size):\n",
        "        \"\"\"\n",
        "        Reconstruct the image from the patches by averaging overlapping regions.\n",
        "        Input patches: [B, num_patches, embed_dim]\n",
        "        img_size: (img_size, img_size)  # original size of the input image\n",
        "\n",
        "        Output images: [B, img_size, img_size]\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        images = self.deconv(patches)\n",
        "        reconstructed_image = images.view(patches.size(0), img_size, img_size)\n",
        "        ######## END TODO ########\n",
        "\n",
        "        return reconstructed_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "d3Shm0MZGWji"
      },
      "outputs": [],
      "source": [
        "class PatchVAE(nn.Module):\n",
        "    def __init__(self, patch_size, img_channels, img_size,\n",
        "                  embed_dim=1024, latent_dim=512, stride=8):\n",
        "        super(PatchVAE, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.img_size = img_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Patch embedding layer (Patchify the image)\n",
        "        self.patch_embed = PatchEmbed(patch_size=patch_size, stride=stride, channels=img_channels, embed_dim=embed_dim, img_size=img_size)\n",
        "        self.num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(embed_dim, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.conv_mu = nn.Conv2d(128, latent_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv_logvar = nn.Conv2d(128, latent_dim, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_input = nn.Conv2d(latent_dim, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ConvTranspose2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ConvTranspose2d(256, embed_dim, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def encode(self, patches):\n",
        "        \"\"\"\n",
        "        Encode the patch embeddings into latent space (mu and logvar).\n",
        "        Args:\n",
        "            patches: Patch embeddings of shape [B, num_patches, embed_dim].\n",
        "        \"\"\"\n",
        "        # convert patch embeddings into latent variables (mu and logvar) through a convolutional\n",
        "        #  encoder. Rearrange patches to treat them as channels before passing them through the encoder.\n",
        "        ######## BEGIN TODO ########\n",
        "        print(\"self.embed_dim\", self.embed_dim)\n",
        "        print(\"patches shape\", patches.shape)\n",
        "        patches = patches.reshape(patches.size(0), self.embed_dim, 1, self.num_patches)\n",
        "        patches = self.encoder(patches)\n",
        "        print(\"pass\")\n",
        "        mu = self.conv_mu(patches)\n",
        "        logvar = self.conv_logvar(patches)\n",
        "        ######## END TODO ########\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"\n",
        "        Reparameterization trick to sample from N(mu, var) using N(0,1).\n",
        "        Args:\n",
        "            mu: Mean of the latent distribution.\n",
        "            logvar: Log variance of the latent distribution.\n",
        "        \"\"\"\n",
        "        # applied the reparameterization trick `z = mu + eps * std`\n",
        "        #  where `eps` is sampled from a standard normal distribution.\n",
        "        ######## BEGIN TODO ########\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        ######## END TODO ########\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"\n",
        "        Decode the latent variable z back to patch embeddings.\n",
        "        Args:\n",
        "            z: Latent variable of shape [B, latent_dim, 1, num_patches].\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # convert the latent variable z back into patch embeddings, then econstruct the original image from these patches\n",
        "        input = self.decoder_input(z)\n",
        "        patch_recon = self.decoder(input)\n",
        "        ######## END TODO ########\n",
        "        return rearrange(patch_recon, 'b c 1 p -> b p c')  # Back to (B, num_patches, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the VAE. Patchify the input, encode into latent space, reparameterize, and decode.\n",
        "        Args:\n",
        "            x: Input image of shape [B, C, img_size, img_size].\n",
        "        \"\"\"\n",
        "        # method first patchifies the image, encode, reparameterize, decode, and finally reconstruct the image\n",
        "        ######## BEGIN TODO ########\n",
        "        patches = self.patch_embed(x)\n",
        "        print(\"patches shape\", patches.shape)\n",
        "        mu, logvar = self.encode(patches) #encode\n",
        "        z = self.reparameterize(mu, logvar) #reparameterize\n",
        "        decoded_patch = self.decode(z) #decode\n",
        "        recon_image = self.patch_embed.reconstruct(decoded_patch, self.img_size) #reconstruct\n",
        "        ######## END TODO ########\n",
        "\n",
        "        return recon_image, mu, logvar\n",
        "\n",
        "    def compute_loss(self, recon_image, original_image, mu, logvar):\n",
        "        \"\"\"\n",
        "        Compute the VAE loss, which consists of the reconstruction loss and KL divergence.\n",
        "        Args:\n",
        "            recon_image: Reconstructed image.\n",
        "            original_image: Original input image.\n",
        "            mu: Mean of the latent distribution.\n",
        "            logvar: Log variance of the latent distribution.\n",
        "        Returns:\n",
        "            loss (Tensor): Total loss (reconstruction loss + KL divergence).\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # calculates the reconstruction loss (Mean Squared Error) and the KL divergence loss to encourage the latent space to \n",
        "        # follow a normal distribution. Combine these losses to get the total VAE loss.\n",
        "        mean_squared_l = nn.MSELoss(reduction='sum')\n",
        "        recon_loss = mean_squared_l(recon_image, original_image)\n",
        "        kl_loss = -0.5 * torch.sum(1 + logvar - torch.pow(mu, 2) - torch.exp(logvar))\n",
        "        ######## END TODO ########\n",
        "        return recon_loss, kl_loss\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        \"\"\"\n",
        "        Generate random samples from the learned distribution.\n",
        "        Args:\n",
        "            num_samples (int): Number of samples to generate.\n",
        "        Returns:\n",
        "            samples (Tensor): Generated\n",
        "        \"\"\"\n",
        "        # generates new random images from the learned latent space. Random latent vector `z` \n",
        "        # are sampled, decoded into patches, and reconstructed into full images.\n",
        "        ######## BEGIN TODO ########\n",
        "        # random laten vector z\n",
        "        z = torch.randn(num_samples, self.latent_dim, 1, self.num_patches)\n",
        "        decoded_patch = self.decode(z) # decode\n",
        "        sample_images = self.patch_embed.reconstruct(decoded_patch, self.img_size) # reconstruct\n",
        "        ######## END TODO ########\n",
        "        return sample_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "EVwMoGte_bfc"
      },
      "outputs": [],
      "source": [
        "def train_patchvae(model, dataloader, optimizer, device, epochs=10, print_interval=100, checkpoint_path='best_model.pth'):\n",
        "    \"\"\"\n",
        "    Training loop for the PatchVAE model with visualization of reconstructed images and saving the best model checkpoint.\n",
        "\n",
        "    Args:\n",
        "        model: The PatchVAE model to be trained.\n",
        "        dataloader: Dataloader for the training data.\n",
        "        optimizer: Optimizer for updating the model parameters.\n",
        "        device: Device (CPU or GPU) on which the training will run.\n",
        "        epochs: Number of training epochs.\n",
        "        print_interval: Interval at which the loss will be printed during training.\n",
        "        checkpoint_path: Path to save the best model checkpoint.\n",
        "\n",
        "    Returns:\n",
        "        losses: List of training losses for each epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    losses = []\n",
        "    best_loss = float('inf')  # Initialize with a large value\n",
        "    initial_beta = 0.0005  # Start with a small KL weight\n",
        "    final_beta = 1.0    # Gradually increase to full KL weight\n",
        "\n",
        "    from datetime import datetime\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "    writer = SummaryWriter(f'logs/patch_vae_{timestamp}')\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "######## BEGIN TODO ########\n",
        "        model.train(True)\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), position=0, leave=True, desc=f'Epoch: {epoch}/{epochs}')\n",
        "        beta = min(final_beta, initial_beta + (epoch * ((final_beta - initial_beta) / epochs)))\n",
        "\n",
        "        for i, data in progress_bar:\n",
        "            print(data.shape)\n",
        "            img = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            print(\"input img shape\", img.shape)\n",
        "            recon_img, mu, logvar = model(img)\n",
        "            recon_loss, kl_loss = model.compute_loss(recon_img, img, mu, logvar)\n",
        "            loss = recon_loss + beta*kl_loss\n",
        "            loss.backward() # not sure\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            progress_bar.set_postfix(\n",
        "                {\n",
        "                    \"loss\": f\"{loss:.3f}\",\n",
        "                    \"kl_loss\": f\"{kl_loss:.3f}\",\n",
        "                    \"recon_loss\": f\"{recon_loss:.3f}\",\n",
        "                    \"variance\": f\"{logvar.exp().mean().item():.3f}\",\n",
        "                }\n",
        "            )\n",
        "            writer.add_scalar('Train_Loss/step', loss.item(), epoch * len(dataloader) + i)\n",
        "            writer.add_scalar('KL_Loss/step', kl_loss.item(), epoch * len(dataloader) + i)\n",
        "            writer.add_scalar('Recon_Loss/step', recon_loss.item(), epoch * len(dataloader) + i)\n",
        "\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        losses.append(avg_loss)\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "        rand_indexs = torch.randint(0, len(dataloader), (4,))\n",
        "        test_images = next(iter(dataloader))[rand_indexs]\n",
        "        test_images = test_images.to(device)\n",
        "        recon_images = []\n",
        "        val_losses = []\n",
        "        val_recon_losses = []\n",
        "        val_kl_losses = []\n",
        "        val_variances = []\n",
        "        \n",
        "        for img in test_images:\n",
        "            recon_img, mu, logvar = model(img.unsqueeze(0))\n",
        "            recon_images.append(recon_img)\n",
        "            recon_loss, kl_loss = model.compute_loss(recon_img, img.unsqueeze(0), mu, logvar)\n",
        "            val_losses.append(recon_loss + beta*kl_loss)\n",
        "            val_recon_losses.append(recon_loss)\n",
        "            val_kl_losses.append(kl_loss)\n",
        "            val_variances.append(logvar.exp().mean().item())\n",
        "            \n",
        "        recon_images = torch.cat(recon_images, dim=0)\n",
        "        val_loss = np.mean(val_losses)\n",
        "        val_recon_loss = np.mean(val_recon_losses)\n",
        "        val_kl_loss = np.mean(val_kl_losses)\n",
        "        val_variance = np.mean(val_variances)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {val_loss:.3f}, Recon Loss: {val_recon_loss:.3f}, KL Loss: {val_kl_loss:.3f}, Variance: {val_variance:.3f}\")\n",
        "        writer.add_scalars(\"Training vs Validation Loss\", {\"Train\": avg_loss, \"Validation\": val_loss}, epoch)\n",
        "        visualize_reconstruction(test_images, recon_images, epoch)\n",
        "        \n",
        "        writer.flush() # Write to disk\n",
        "    ######## END TODO ########\n",
        "    return losses\n",
        "\n",
        "def visualize_reconstruction(original_images, recon_images, epoch):\n",
        "    \"\"\"\n",
        "    Visualize original and reconstructed images side by side.\n",
        "\n",
        "    Args:\n",
        "        original_images: Batch of original images (values between 0 and 1).\n",
        "        recon_images: Batch of reconstructed images (values between 0 and 1).\n",
        "        epoch: Current epoch number.\n",
        "    \"\"\"\n",
        "    num_images = min(4, original_images.size(0))  # Visualize at most 4 images\n",
        "    fig, axes = plt.subplots(2, num_images, figsize=(12, 4))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Original images (ensure values are between 0 and 1)\n",
        "        orig_img = original_images[i].permute(1, 2, 0).cpu().numpy()  # Convert to (H, W, C)\n",
        "        orig_img = np.clip(orig_img, 0, 1)  # Clip values to [0, 1] range\n",
        "\n",
        "        # Reconstructed images (ensure values are between 0 and 1)\n",
        "        recon_img = recon_images[i].permute(1, 2, 0).cpu().numpy()  # Convert to (H, W, C)\n",
        "        recon_img = np.clip(recon_img, 0, 1)  # Clip values to [0, 1] range\n",
        "\n",
        "        # Display original images\n",
        "        axes[0, i].imshow(orig_img)\n",
        "        axes[0, i].set_title(\"Original\")\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Display reconstructed images\n",
        "        axes[1, i].imshow(recon_img)\n",
        "        axes[1, i].set_title(\"Reconstructed\")\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.suptitle(f\"Reconstruction at Epoch {epoch+1}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "4Ninfhma_ht0",
        "outputId": "30e8451f-5613-45b8-fedf-f51fabbb7905"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 0/15:   0%|          | 0/79 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([34, 3, 32, 32])\n",
            "input img shape torch.Size([34, 3, 32, 32])\n",
            "patches shape torch.Size([34, 64, 16])\n",
            "self.embed_dim 64\n",
            "patches shape torch.Size([34, 64, 16])\n",
            "pass\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa17be1d300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/yufeiyang/24_fall/machine_perception/P4/project4/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/yufeiyang/24_fall/machine_perception/P4/project4/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1568, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ],
      "source": [
        "# Train PatchVAE\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resume_training = False\n",
        "checkpoint_path = 'best_vae_model.pth'\n",
        "\n",
        "\n",
        "vae = PatchVAE(patch_size=8, img_channels=3, img_size=32, embed_dim=64, latent_dim=128).to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-4)\n",
        "\n",
        "if resume_training:\n",
        "    vae.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "train_patchvae(vae, dataloader, optimizer, device, epochs=15, checkpoint_path='best_vae_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKWsxg1ZtmNX"
      },
      "outputs": [],
      "source": [
        "# Generate images by sampling from latent space\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint_path = 'best_vae_model.pth'\n",
        "vae = PatchVAE(patch_size=8, img_channels=3, img_size=32, embed_dim=64, latent_dim=128).to(device)\n",
        "vae.load_state_dict(torch.load(checkpoint_path))\n",
        "vae.eval()\n",
        "\n",
        "samples = vae.sample(4)\n",
        "samples = samples.permute(0, 2, 3, 1).cpu().numpy()\n",
        "print(\"samples min:\", np.min(samples))\n",
        "print(\"samples max:\", np.max(samples))\n",
        "\n",
        "samples = np.clip(samples, 0, 1)\n",
        "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
        "for i in range(4):\n",
        "    axes[i].imshow(samples[i])\n",
        "    axes[i].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJjs9i0npQZg"
      },
      "source": [
        "# Part B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kS2hsDujYJg"
      },
      "source": [
        "# Diffusion Transformer (DiT)\n",
        "\n",
        "\n",
        "We will train a DiT that operates in the patch latent space on the same dataset. The input image is decomposed into patches and processed by several DiT blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP4y-UwHI0xU"
      },
      "source": [
        "### Implement Noise Scheduler\n",
        "\n",
        "\n",
        "You will implement parts of the [DDPM scheduler](https://huggingface.co/docs/diffusers/en/api/schedulers/ddpm) using the following steps.\n",
        "\n",
        "1. Define $\\beta$ as a tensor of $N$ linearly spaced values between $\\beta_{start}$ and $\\beta_{end}$.\n",
        "\n",
        "$$\\beta = \\left[\\beta_{\\text{start}}, \\beta_{\\text{start}} + \\frac{\\beta_{\\text{end}} - \\beta_{\\text{start}}}{N - 1}, \\ldots, \\beta_{\\text{end}}\\right]$$\n",
        "\n",
        "2. Calculate $\\sigma$ using the cumulative product of $1 - \\beta$.\n",
        "\n",
        "$$\\sigma = \\sqrt{\\frac{1}{\\prod_{i=0}^{N-1}(1 - \\beta_i)} - 1}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1B_PlLWyHTQk"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 2 (2296081815.py, line 7)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[34], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __getitem__(self, i) -> torch.FloatTensor:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 2\n"
          ]
        }
      ],
      "source": [
        "class DDPMScheduler:\n",
        "    def __init__(self, N: int=1000, beta_start: float=0.0001, beta_end: float=0.02):\n",
        "        ######## BEGIN TODO ########\n",
        "        # self.sigmas = pass\n",
        "        ######## END TODO ########\n",
        "\n",
        "    def __getitem__(self, i) -> torch.FloatTensor:\n",
        "        return self.sigmas[i]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sigmas)\n",
        "\n",
        "    def sample_sigmas(self, steps: int) -> torch.FloatTensor:\n",
        "        indices = list((len(self) * (1 - np.arange(0, steps)/steps)).round().astype(np.int64) - 1)\n",
        "        return self[indices + [0]]\n",
        "\n",
        "    def sample_batch(self, x0: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        batchsize = x0.shape[0]\n",
        "        return self[torch.randint(len(self), (batchsize,))].to(x0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWBzy8OKJdhW"
      },
      "source": [
        "### Implement DiT Block\n",
        "\n",
        "You will implement the DiT Block with adaLN-Zero, as illustrated in Figure 3 of the Diffusion Transformer paper. Specifically, you will need to implement Multi-Head Self-Attention, Modulation, and Adaptive Layer Norm for the DiT Block, and we will provide you with the code structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PodOL2SCae6M"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, head_dim, num_heads=8, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            head_dim (int): Dimensionality of each attention head\n",
        "            num_heads (int): Number of attention heads\n",
        "            qkv_bias (bool): Whether to include bias in the QKV layer\n",
        "        \"\"\"\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "        ######## BEGIN TODO ########\n",
        "        # self.qkv = pass\n",
        "        # self.proj = pass\n",
        "        ######## END TODO #######\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x (Tensor): Input tensor of shape (B, N, D), where:\n",
        "                - B: Batch size\n",
        "                - N: Number of tokens\n",
        "                - D: Dimensionality (D = num_heads * head_dim)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (B, N, D) after applying attention and projection\n",
        "\n",
        "        You may use F.scaled_dot_product_attention\n",
        "        \"\"\"\n",
        "        # (B, N, D) -> (B, N, D)\n",
        "        # N = H * W / patch_size**2, D = num_heads * head_dim\n",
        "\n",
        "        ######## BEGIN TODO ########\n",
        "        pass\n",
        "        ######## END TODO #######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JetvO5PnMjmH"
      },
      "outputs": [],
      "source": [
        "class Modulation(nn.Module):\n",
        "    def __init__(self, dim, n):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        1. self.proj is constructed as a sequence of operations:\n",
        "            - A SiLU (Sigmoid-Weighted Linear Unit) activation is applied to the input\n",
        "            - A linear layer transforms the input from dim to n * dim\n",
        "        2. The last layer's weights and biases are initialized to zero\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # self.proj = pass\n",
        "        ######## END TODO #######\n",
        "\n",
        "\n",
        "    def forward(self, y):\n",
        "        \"\"\"\n",
        "        1. self.proj(y) applies the defined projection, resulting in n * dim\n",
        "        2. Split the output tensor into n equal parts along dimension 1\n",
        "        3. Each chunk `m` has the shape dim and represents a separate modulation component\n",
        "        4. m.unsqueeze(1) adds a new dimension at index 1, necessary for future computations\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        pass\n",
        "        ######## END TODO #######\n",
        "\n",
        "\n",
        "class AdaptiveLayerNorm(nn.LayerNorm):\n",
        "    def __init__(self, dim, **kwargs):\n",
        "        super().__init__(dim, **kwargs)\n",
        "        \"\"\"\n",
        "        Initialize an instance of Modulation with dim and n = 2\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # self.modulation = pass\n",
        "        ######## END TODO #######\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        1. Obtain (scale, shift) tensors from applying modulation on input y\n",
        "        2. Apply LayerNorm on input x, which we will denote as LayerNorm(x)\n",
        "        3. Compute AdaptiveLayerNorm as `LayerNorm(x) * (1 + scale) + shift`\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        pass\n",
        "        ######## END TODO #######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDwvEVAnJdBU"
      },
      "outputs": [],
      "source": [
        "class DiTBlock(nn.Module):\n",
        "    def __init__(self, head_dim, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        dim = head_dim * num_heads\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "\n",
        "        self.norm1 = AdaptiveLayerNorm(dim, elementwise_affine=False, eps=1e-6)\n",
        "        self.attn = Attention(head_dim, num_heads=num_heads, qkv_bias=True)\n",
        "        self.norm2 = AdaptiveLayerNorm(dim, elementwise_affine=False, eps=1e-6)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim, bias=True),\n",
        "            nn.GELU(approximate=\"tanh\"),\n",
        "            nn.Linear(mlp_hidden_dim, dim, bias=True),\n",
        "        )\n",
        "        self.scale_modulation = Modulation(dim, 2)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # (B, N, D), (B, D) -> (B, N, D)\n",
        "        # N = H * W / patch_size**2, D = num_heads * head_dim\n",
        "        gate_msa, gate_mlp = self.scale_modulation(y)\n",
        "        x = x + gate_msa * self.attn(self.norm1(x, y))\n",
        "        x = x + gate_mlp * self.mlp(self.norm2(x, y))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D90m_RQxM3IG"
      },
      "outputs": [],
      "source": [
        "def sigma_log_scale(batches, sigma, scaling_factor):\n",
        "    if sigma.shape == torch.Size([]):\n",
        "        sigma = sigma.unsqueeze(0).repeat(batches)\n",
        "    else:\n",
        "        assert sigma.shape == (batches,), 'sigma.shape == [] or [batches]!'\n",
        "    return torch.log(sigma)*scaling_factor\n",
        "\n",
        "\n",
        "def get_sigma_embeds(batches, sigma, scaling_factor=0.5):\n",
        "    s = sigma_log_scale(batches, sigma, scaling_factor).unsqueeze(1)\n",
        "    return torch.cat([torch.sin(s), torch.cos(s)], dim=1)\n",
        "\n",
        "\n",
        "class SigmaEmbedderSinCos(nn.Module):\n",
        "    def __init__(self, hidden_size, scaling_factor=0.5):\n",
        "        super().__init__()\n",
        "        ######## BEGIN TODO ########\n",
        "        pass\n",
        "        ######## END TODO #######\n",
        "\n",
        "\n",
        "    def forward(self, batches, sigma):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        pass\n",
        "        ######## END TODO #######"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVHfC59UqOhg"
      },
      "source": [
        "### Helper Functions\n",
        "\n",
        "We provide you with the following helper functions for training DiT. No implementation needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kITU_H73MvIo"
      },
      "outputs": [],
      "source": [
        "def get_pos_embed(in_dim, patch_size, dim, N=10000):\n",
        "    n = in_dim // patch_size                                          # Number of patches per side\n",
        "    assert dim % 4 == 0, 'Embedding dimension must be multiple of 4!'\n",
        "    omega = 1/N**np.linspace(0, 1, dim // 4, endpoint=False)          # [dim/4]\n",
        "    freqs = np.outer(np.arange(n), omega)                             # [n, dim/4]\n",
        "    embeds = repeat(np.stack([np.sin(freqs), np.cos(freqs)]),\n",
        "                       ' b n d -> b n k d', k=n)                      # [2, n, n, dim/4]\n",
        "    embeds_2d = np.concatenate([\n",
        "        rearrange(embeds, 'b n k d -> (k n) (b d)'),                  # [n*n, dim/2]\n",
        "        rearrange(embeds, 'b n k d -> (n k) (b d)'),                  # [n*n, dim/2]\n",
        "    ], axis=1)                                                        # [n*n, dim]\n",
        "    return nn.Parameter(torch.tensor(embeds_2d).float().unsqueeze(0), # [1, n*n, dim]\n",
        "                        requires_grad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk5_8SFWNNpP"
      },
      "outputs": [],
      "source": [
        "class ModelMixin:\n",
        "    def rand_input(self, batchsize):\n",
        "        assert hasattr(self, 'input_dims'), 'Model must have \"input_dims\" attribute!'\n",
        "        return torch.randn((batchsize,) + self.input_dims)\n",
        "\n",
        "    # Currently predicts eps, override following methods to predict, for example, x0\n",
        "    def get_loss(self, x0, sigma, eps):\n",
        "        return nn.MSELoss()(eps, self(x0 + sigma * eps, sigma))\n",
        "\n",
        "    def predict_eps(self, x, sigma):\n",
        "        return self(x, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Mlu5v6qVkx"
      },
      "source": [
        "### Implement Entire DiT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_QETY8XJY4A"
      },
      "outputs": [],
      "source": [
        "class DiT(nn.Module, ModelMixin):\n",
        "    def __init__(self, in_dim=32, channels=3, patch_size=2, depth=12,\n",
        "                 head_dim=64, num_heads=6, mlp_ratio=4.0, sig_embed_factor=0.5,\n",
        "                 sig_embed_class=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.channels = channels\n",
        "        self.patch_size = patch_size\n",
        "        self.input_dims = (channels, in_dim, in_dim)\n",
        "\n",
        "        dim = head_dim * num_heads\n",
        "\n",
        "        self.pos_embed = get_pos_embed(in_dim, patch_size, dim)\n",
        "        self.x_embed = PatchEmbed(img_size=in_dim, patch_size=patch_size, stride=patch_size,\n",
        "                                      channels=channels, embed_dim=dim, bias=True)\n",
        "        self.sig_embed = (sig_embed_class or SigmaEmbedderSinCos)(\n",
        "            dim, scaling_factor=sig_embed_factor\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DiTBlock(head_dim, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.final_norm = AdaptiveLayerNorm(dim, elementwise_affine=False, eps=1e-6)\n",
        "        self.final_linear = nn.Linear(dim, patch_size**2 * channels)\n",
        "        self.init()\n",
        "\n",
        "\n",
        "    def init(self):\n",
        "        # Initialize transformer layers\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "        self.apply(_basic_init)\n",
        "\n",
        "        # Initialize sigma embedding MLP\n",
        "        nn.init.normal_(self.sig_embed.mlp[0].weight, std=0.02)\n",
        "        nn.init.normal_(self.sig_embed.mlp[2].weight, std=0.02)\n",
        "\n",
        "        # Zero-out output layers\n",
        "        nn.init.constant_(self.final_linear.weight, 0)\n",
        "        nn.init.constant_(self.final_linear.bias, 0)\n",
        "\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        Input x has size [B, N, (patch_size)**2 * C] where N is number of patches\n",
        "\n",
        "        Return image with size [B, C, H, W]\n",
        "        \"\"\"\n",
        "        # (B, N, patchsize**2 * channels) -> (B, channels, H, W)\n",
        "        patches = self.in_dim // self.patch_size\n",
        "\n",
        "        ######## BEGIN TODO ########\n",
        "        pass\n",
        "        ######## END TODO ########\n",
        "\n",
        "\n",
        "    def forward(self, x, sigma):\n",
        "\n",
        "        ######## BEGIN TODO ########\n",
        "        pass\n",
        "        ######## END TODO ########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "307IzvkcNbFZ"
      },
      "source": [
        "### Train DiT\n",
        "\n",
        "Here, we provide the pl LightningModule class for you. You only need to implement the `sample_image` function to generate new images and `training_step` function. You should use PatchVAE to generate latent encodings and train diffusion model on the latent encodings. In `sample_image` function, you should use DiT to generate latent encoding and use PatchVAE decoder to decode. The PatchVAE model should be fixed during training.\n",
        "\n",
        "Hint: use `get_loss` and `generate_train_sample` functions !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwEBKBdA3C2d"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "\n",
        "class DiffusionModel(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # patch_size=8, img_channels=3, img_size=32, embed_dim=64, latent_dim=128\n",
        "\n",
        "    self.img_shape = 32\n",
        "    self.vae_patch_size = 8\n",
        "    self.vae_stride = 8\n",
        "    self.latent_dim = 128\n",
        "\n",
        "    # VAE Encoder for latent space\n",
        "    self.patch_vae = PatchVAE(patch_size=self.vae_patch_size, img_channels=3, stride=self.vae_stride,\n",
        "                                  img_size=self.img_shape, embed_dim=64, latent_dim=self.latent_dim)\n",
        "\n",
        "    self.in_dim = (self.img_shape - self.vae_patch_size) // self.vae_stride + 1\n",
        "\n",
        "    # for VAE latent\n",
        "    self.model = DiT(in_dim=self.in_dim, channels=self.latent_dim,\n",
        "                      patch_size=1, depth=6,\n",
        "                      head_dim=self.latent_dim, num_heads=6, mlp_ratio=4.0)\n",
        "\n",
        "    self.schedule = DDPMScheduler(beta_start=0.0001, beta_end=0.02, N=1000)\n",
        "\n",
        "  def on_training_epoch_start(self):\n",
        "    \"\"\" Fix patch vae \"\"\"\n",
        "    for param in self.patch_vae.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  def generate_train_sample(self, x0: torch.FloatTensor):\n",
        "    \"\"\" Generate train samples\n",
        "\n",
        "    Args:\n",
        "      x0: torch.Tensor in shape ()\n",
        "    \"\"\"\n",
        "    sigma = self.schedule.sample_batch(x0)\n",
        "    while len(sigma.shape) < len(x0.shape):\n",
        "        sigma = sigma.unsqueeze(-1)\n",
        "    eps = torch.randn_like(x0)\n",
        "    return sigma, eps\n",
        "\n",
        "  # provide here\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    \"\"\" Training Step \"\"\"\n",
        "    x0 = batch\n",
        "\n",
        "    ### YOUR CODE BEGINS ###\n",
        "    pass\n",
        "    ## YOUR CODE ENDS ###\n",
        "\n",
        "    self.log('train_loss', loss, on_step=True, prog_bar=True)\n",
        "    return loss\n",
        "\n",
        "  def sample_image(self, gam = 1.6, mu = 0., xt = None, batchsize = 4):\n",
        "    \"\"\" Function to generate image samples\n",
        "\n",
        "    Args:\n",
        "      gam: float, suggested to use gam >= 1\n",
        "      mu: float, requires mu in [0, 1)\n",
        "      xt: torch.Tensor, optional, default None\n",
        "      batchsize: int, optional, default 4\n",
        "    Return:\n",
        "      torch.Tensor in shape (batchsize, 1, 28, 28)\n",
        "    \"\"\"\n",
        "    sigmas = self.schedule.sample_sigmas(20)\n",
        "\n",
        "    if xt is None:\n",
        "        xt = self.model.rand_input(batchsize).to(self.device) * sigmas[0]\n",
        "    else:\n",
        "        batchsize = xt.shape[0]\n",
        "\n",
        "    ######### BEGIN TODO ########\n",
        "    pass\n",
        "    ######## END TODO ########\n",
        "\n",
        "    return recon_image\n",
        "\n",
        "  def on_train_epoch_end(self) -> None:\n",
        "    if self.current_epoch % 10 == 0:\n",
        "      batchsize = 4\n",
        "      imgs = self.sample_image(batchsize=batchsize)\n",
        "\n",
        "      fig, axes = plt.subplots(1, batchsize, figsize=(batchsize*4, 2))\n",
        "      for i in range(batchsize):\n",
        "        axes[i].imshow(imgs[i].permute(1, 2, 0).detach().cpu().numpy())\n",
        "        axes[i].axis('off')\n",
        "\n",
        "      # add title\n",
        "      fig.suptitle(f'Epoch {self.current_epoch}')\n",
        "      fig.savefig(f\"epoch_{self.current_epoch}.png\")\n",
        "      plt.close()\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "\n",
        "model = DiffusionModel()\n",
        "# load patch vae\n",
        "model.patch_vae.load_state_dict(torch.load(\"./best_vae_model.pth\"))\n",
        "trainer = pl.Trainer(max_epochs=200)\n",
        "trainer.fit(model, dataloader)\n",
        "torch.save(model.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M_CfUWhqi4v"
      },
      "source": [
        "### Sample and Evaluate DiT\n",
        "\n",
        "In addition to implementing the code above, please complete the following:\n",
        "1. Plot loss over epochs. Show that loss decreases over epochs.\n",
        "2. Generate new samples using your DiT checkpoint. Show that samples look reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ea2InK4Q6gq",
        "outputId": "92baef13-8e19-4990-c857-e943b7c1f505"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-755576d5c2bf>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model.pt\"))\n",
            "100%|██████████| 251/251 [00:57<00:00,  4.38it/s]\n",
            "100%|██████████| 251/251 [00:04<00:00, 51.98it/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Discard labels and only output data\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, dataset, fn):\n",
        "        self.dataset = dataset\n",
        "        self.fn = fn\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    def __getitem__(self, i):\n",
        "        return self.fn(self.dataset[i])\n",
        "\n",
        "# generate 2k images and score in the tensor\n",
        "total_image = 2000\n",
        "batch_size = 8\n",
        "iterations = 2000 // batch_size + 1\n",
        "model.load_state_dict(torch.load(\"model.pt\"))\n",
        "model.eval()\n",
        "model.cuda()\n",
        "\n",
        "sampled_images = np.zeros((batch_size * iterations, 3, 32, 32), dtype=np.float32)\n",
        "for i in tqdm(range(iterations)):\n",
        "  imgs = model.sample_image(batchsize=batch_size)\n",
        "  sampled_images[i*batch_size:(i+1)*batch_size] = imgs.detach().cpu().numpy()\n",
        "sampled_images = sampled_images[:total_image]\n",
        "mydataset = MyDataset(sampled_images, lambda x: x)\n",
        "\n",
        "patch_vae_images = np.zeros((batch_size * iterations, 3, 32, 32), dtype=np.float32)\n",
        "for i in tqdm(range(iterations)):\n",
        "  imgs = model.patch_vae.sample(batch_size)\n",
        "  patch_vae_images[i*batch_size:(i+1)*batch_size] = imgs.detach().cpu().numpy()\n",
        "patch_vae_images = patch_vae_images[:total_image]\n",
        "patch_vae_dataset = MyDataset(patch_vae_images, lambda x: x)\n",
        "\n",
        "celebdataset = dataset = Subset(dataloader.dataset, torch.arange(2000))  # Limit dataset size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBD3AYfx6IsL"
      },
      "source": [
        "3. Compute FID between images in CelebA dataset and:\n",
        "    i. Images generated from PatchVAE.\n",
        "    ii. Images generated from Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37LcMnFLfnSa"
      },
      "outputs": [],
      "source": [
        "! pip install pytorch-fid scipy==1.11.1\n",
        "from pytorch_fid.inception import InceptionV3\n",
        "import scipy.linalg as linalg\n",
        "\n",
        "def build_feature_table(dataset, model, batch_size, dim, device):\n",
        "    '''\n",
        "    Argms:\n",
        "    Input:\n",
        "        dataset: pytorch dataset, you want to evaluate IS score on\n",
        "        model: Inception network v3\n",
        "        batch_size: int number\n",
        "        dim: for IS computation, dim should be 1000 as the final softmax out put dimension\n",
        "        device: device type torch.device(\"cuda:0\") or torch.device(\"cpu\")\n",
        "    Output:\n",
        "        feature_table: (n,dim) numpy matrix\n",
        "    '''\n",
        "    # model enter eval mode\n",
        "    model.eval()\n",
        "    # initalize the dataloader\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
        "    n = len(dataset)\n",
        "    idx_counter = 0\n",
        "    # feature table\n",
        "    feature_table = np.zeros((n, dim))\n",
        "\n",
        "    for i, data in tqdm(enumerate(dataloader, 0)):\n",
        "        image = data.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(image)[0]\n",
        "            pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
        "            feature_table[idx_counter:idx_counter+pred.shape[0]] = pred\n",
        "            idx_counter += len(pred)\n",
        "\n",
        "    return feature_table\n",
        "\n",
        "def compute_stat(feature_table):\n",
        "    '''\n",
        "    Argms:\n",
        "    Input:\n",
        "        feature_table: (n,dim) numpy matrix\n",
        "    Output:\n",
        "        mu: mean along row dimension\n",
        "        sigma: covarance matrix of dataset\n",
        "    '''\n",
        "    # compute mean and sigma based on activation table\n",
        "    mu = np.mean(feature_table, axis=0)\n",
        "    sigma = np.cov(feature_table, rowvar=False)\n",
        "\n",
        "    return mu, sigma\n",
        "\n",
        "def compute_FID(mu_1, sigma_1, mu_2, sigma_2, eps=1e-6):\n",
        "    '''\n",
        "    Argms:\n",
        "    Input:\n",
        "        mu_1: mean vector we get for dataset1\n",
        "        sigma_1: covariance matrix for dataset1\n",
        "        mu_2: mean vector we get for dataset2\n",
        "        sigma_2: covariance matrix for dataset1\n",
        "    Output:\n",
        "        FID score: float\n",
        "    '''\n",
        "\n",
        "    # compute mu difference\n",
        "\n",
        "    # compute square root of Sigma1*Sigma2 using \"linalg.sqrtm\" from scipy\n",
        "    # please name the resulting matrix as covmean\n",
        "\n",
        "    ######### BEGIN TODO ########\n",
        "    pass\n",
        "    ######## END TODO ########\n",
        "\n",
        "\n",
        "    # The following block take care of imagionary part of covmean\n",
        "    #################################################################\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma_1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma_1 + offset).dot(sigma_2 + offset))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "    #################################################################\n",
        "\n",
        "    ######### BEGIN TODO ########\n",
        "    pass\n",
        "    ######## END TODO ########\n",
        "\n",
        "\n",
        "\n",
        "    return FID\n",
        "\n",
        "def FID(dataset_1, dataset_2, device, batch_size=64, dim=2048, block_idx = 3):\n",
        "    '''\n",
        "    Argms:\n",
        "    Input:\n",
        "        dataset_1: pytorch dataset\n",
        "        dataset_2: pytorch dataset\n",
        "        device: device type torch.device(\"cuda:0\") or torch.device(\"cpu\")\n",
        "        batch_size: int number\n",
        "        dim: for IS computation, dim should be 1000 as the final softmax out put dimension\n",
        "        block_idx: the block stage index we want to use in inception module\n",
        "    Output:\n",
        "        FID_score: float\n",
        "    '''\n",
        "    # load InveptionV3 model\n",
        "    model = InceptionV3([block_idx]).to(device)\n",
        "\n",
        "    ## build up the feature table\n",
        "    feature_table_1 = build_feature_table(dataset_1, model, batch_size, dim, device)\n",
        "    feature_table_2 = build_feature_table(dataset_2, model, batch_size, dim, device)\n",
        "\n",
        "\n",
        "    ## compute mu, sigma for dataset 1&2\n",
        "    mu_1, sigma_1 = compute_stat(feature_table_1)\n",
        "    mu_2, sigma_2 = compute_stat(feature_table_2)\n",
        "\n",
        "    ## FID score computation\n",
        "    FID_score = compute_FID(mu_1, sigma_1, mu_2, sigma_2, eps=1e-6)\n",
        "\n",
        "    return FID_score\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "FID_score = FID(celebdataset, celebdataset, device)\n",
        "print('FID_score between CelebA and itself:', FID_score)\n",
        "\n",
        "FID_score = FID(celebdataset, mydataset, device)\n",
        "print('FID_score between CelebA and images sampled from Diffusion Model:', FID_score)\n",
        "\n",
        "FID_score = FID(celebdataset, patch_vae_dataset, device)\n",
        "print('FID_score between CelebA and images sampled from PatchVAE:', FID_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jj1sSnbJpMpj",
        "5iLpnEI6H1Cd",
        "AxxFfD3rc2MG"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
