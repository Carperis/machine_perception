{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1_ulf83-jhW"
      },
      "source": [
        "# CIS6800: Project 3: SOLO and FPN (Instance Segmentation)\n",
        "\n",
        "\n",
        "### Instructions:\n",
        "* This is a group assignment with one submission per group. It is expected that each member of the group will contribute to solving each question. Be sure to specify your teammates when you submit to Gradescope! Collaborating with other groups is not permitted.\n",
        "* There is no single answer to most problems in deep learning, therefore the questions will often be underspecified. You need to fill in the blanks and submit a solution that solves the (practical) problem. Document the choices (hyperparameters, features, neural network architectures, etc.) you made where specified.\n",
        "* You may include any code used in previous projects. You may use ChatGPT, but you need to document how you use it.\n",
        "* You should expect that a complete training session should last about 6 hours, so you should start part (b) as early as you can!\n",
        "* To save on compute time, we debugging locally and using colab to train. Or use colab cpu instance for debugging\n",
        "* We should be able to reproduce your results with a single function call or cell evaluation. Please specify the method to do so. You do not, however, need to submit your trained weights.\n",
        "\n",
        "\n",
        "The SOLO paper: https://arxiv.org/pdf/1912.04488.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download data, if the auto download faild, you can maunally download here:\n",
        "# https://drive.google.com/drive/folders/17cT1ywZeKJ5l-FinBwpWv-EbvaMJ9mSa?usp=sharing\n",
        "\n",
        "!gdown --id 1JD3OaHpq_4KCb7ofcPMkknmdEXFHrCcn\n",
        "!gdown --id 1ssRA7yijGLFmJU-ac-lPyUOq7DYzTAS1\n",
        "!gdown --id 1Rpz-ZuQxDwvLyzc0FD9GZxAKlyka3VC5\n",
        "!gdown --id 1ouMFNT1thia8l6P5vcWCY-nweAexLDsB\n",
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPgvajFFMlC6",
        "outputId": "38c76034-bec0-440c-c8cd-c31ed425b35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Access denied with the following error:\n",
            "\n",
            " \tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1JD3OaHpq_4KCb7ofcPMkknmdEXFHrCcn \n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ssRA7yijGLFmJU-ac-lPyUOq7DYzTAS1\n",
            "To: /content/hw3_mycocodata_labels_comp_zlib.npy\n",
            "100% 269k/269k [00:00<00:00, 3.85MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Access denied with the following error:\n",
            "\n",
            " \tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1Rpz-ZuQxDwvLyzc0FD9GZxAKlyka3VC5 \n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Access denied with the following error:\n",
            "\n",
            " \tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1ouMFNT1thia8l6P5vcWCY-nweAexLDsB \n",
            "\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.9.post0-py3-none-any.whl (727 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.7/727.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (17.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.9.0 pytorch-lightning-2.0.9.post0 torchmetrics-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xjZNQ4F-jhe"
      },
      "source": [
        "## Code Structure\n",
        "In this assignment, we are only providing the barest templates for your code structure. If you prefer, you can write and debug most of the components in a regular python and only use Jupyter to train. In this case, you might have several files that you import into this notebook, e.g.\n",
        "* `dataset.py`\n",
        "* `model.py`\n",
        "* `train.py`\n",
        "* `inference.py`\n",
        "\n",
        "This is shown below. All files should be included in your submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJQ7G9Jf-jhg"
      },
      "source": [
        "try:\n",
        "    import dataset\n",
        "    import model\n",
        "    import train\n",
        "    import inference\n",
        "except ModuleNotFoundError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDRG7ySI-jhk"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "253rxTGZ-jhm"
      },
      "source": [
        "Instance segmentation can be thought of as a combination of object detection and semantic segmentation, the former of which you already explored in the previous project. A visulization of this relationship can be seen in fig. 1.\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig1.png\" width=500/></div>\n",
        "<center>Figure 1: An overview of instance segmentation.</center>  \n",
        "\n",
        "In this project, you are tasked with implementing an instance segmentation framework know as SOLO (Segmenting Object by LOcations). In a similar manner to YOLO, SOLO produces mask predictions on a dense grid. This means that, unlike many other segmenation frameworks (e.g. Mask-RCNN), SOLO directly predicts the segmentation mask without proposing bounding box locations. An visual summary of SOLO can be seen in fig. 2 and 3.\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig2.png\" width=200/></div>\n",
        "<center>Figure 2: SOLO.</center>  \n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig3.png\" width=600/></div>\n",
        "<center>Figure 3: SOLO branches.</center>\n",
        "\n",
        "These dense predictions are produced at several different scales using a Feature Pyramid Network (FPN). Using the last few layers of the backbone, we pass the higher level features from the deeper layers back up to larger features scales using lateral connections, shown in fig. 4. The integration of the FPN into your network will be the primary focus of this project.\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig4.png\" width=300/></div>\n",
        "<center>Figure 4: SOLO branches.</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU84IoOl-jho"
      },
      "source": [
        "## Part A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MnlCd9m-jhq"
      },
      "source": [
        "### Dataset\n",
        "#### Dataset Structure\n",
        "The dataset used in this project contains three classes across 3265 images: vehicles, people, and animals. The dataset contains the following elements:\n",
        "\n",
        "* A numpy array of images ($3 \\times 300 \\times 400$).\n",
        "* A numpy array of masks ($300 \\times 400$).\n",
        "* A list of ground truth labels by image.\n",
        "* A list of bounding boxes by image.\n",
        "\n",
        "Note that the mask array is flattened; to determine which masks belong to which image, count the number of labels associated with that image. For example, if the first few images have 3, 2, and 4 labels, masks 0-2 would belong to image 1, masks 3-4 would belong to image 2, etc. The masks are ordered correctly to allow for this.\n",
        "\n",
        "#### Loading and Batching\n",
        "You should apply the following transformations to each image:\n",
        "* Normalize pixel values to $[0,1]$.\n",
        "* Rescale the image to $800 \\times 1066$.\n",
        "* Normalize each channel with means $[0.485, 0.456, 0.406]$ and standard deviations $[0.229, 0.224, 0.225]$.\n",
        "* Zero pad the image to $800 \\times 1088$.\n",
        "\n",
        "Since each image will have a different number of objects, you will have to write your own collation function to properly batch the images. An example collate_fn is shown below, along with expected output dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJYYsbUw-jhs"
      },
      "source": [
        "# images:         (batch_size, 3, 800, 1088)\n",
        "# labels:         list with len: batch_size, each (n_obj,)\n",
        "# masks:          list with len: batch_size, each (n_obj, 800,1088)\n",
        "# bounding_boxes: list with len: batch_size, each (n_obj, 4)\n",
        "def collate_fn(batch):\n",
        "    images, labels, masks, bounding_boxes = list(zip(*batch))\n",
        "    return torch.stack(images), labels, masks, bounding_boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DefEnjjQ-jhu"
      },
      "source": [
        "#### Visualization\n",
        "In order to validate that you are loading the dataset correct, you should plot at least five example images that include the mask, annotations, and bounding boxes. Examples of such images are shown in fig. 5 and 6. Make sure that the color for each class is consistent!\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig5.png\" width=500/></div>\n",
        "<center>Figure 5: Example visualization.</center>\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig6.png\" width=500/></div>\n",
        "<center>Figure 6: Example visualization.</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Rz_Srf-jhw"
      },
      "source": [
        "### Model\n",
        "#### Architecture\n",
        "The model architecture is summarized in fig. 7 and tables 1 and 2.\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig7.png\" width=500/></div>\n",
        "<center>Figure 7: SOLO branch structure.</center>\n",
        "<br>\n",
        "<center>Table 1: Category branch structure.</center>\n",
        "\n",
        "| Layer | Hyperparameters |\n",
        "| :--- | :--- |\n",
        "| conv1 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv2 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv3 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv4 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv5 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv6 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv7 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv_out | Kernel size $= 3\\times3\\times(C-1)$, stride $= 1$, pad $= 1$, bias $= \\text{True}$. <br> Followed by Sigmoid layer. Note $C = 4$ here (number of classes + background). |\n",
        "\n",
        "<br>\n",
        "<center>Table 2: Mask branch structure.</center>\n",
        "\n",
        "| Layer | Hyperparameters |\n",
        "| :--- | :--- |\n",
        "| conv1 | Kernel size $= 3\\times3\\times(256 + 2)$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Two additional input channels represent the $x$ and $y$ positional encoding. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv2 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv3 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv4 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv5 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv6 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv7 | Kernel size $= 3\\times3\\times256$, stride $= 1$, pad $= 1$, bias $= \\text{False}$. <br> Followed by GroupNorm, num_groups $=32$ and ReLU |\n",
        "| conv_out | Kernel size $= 1\\times1\\times(\\text{num_grid})^2$, stride $= 1$, pad $= 0$, bias $= \\text{True}$. <br> Followed by Sigmoid layer. Note that $\\text{num_grid}$ is different for each layer of the FPN. |\n",
        "\n",
        "We will be using a pretrained backbone (which includes an FPN), so you will not have to implement those components. A template for the network with along with default parameters is shown below.\n",
        "\n",
        "#### Feature Pyramid\n",
        "The feature pyramid extracted below has strides $[4,8,16,32,64]$ over the original image. To match the SOLO paper, this should be interpolated to have strides $[8,8,16,32,32]$.\n",
        "\n",
        "#### Target Assignment\n",
        "Some notes about generating the ground truth targets:\n",
        "* The FPN levels can be though of as different grid sizes cut through the image.\n",
        "* You assign each target to a certain FPN level if $\\sqrt{wh}$ from the bounding box falls within the `scale_range` associated with that level. Note that these overlap, so you may assign the same target to multiple levels.\n",
        "* A grid cell should be considered as predicting an object if that grid cell falls into the \"centre region\" of the object.\n",
        " * The centre region of an object is its bounding box scaled by `epsilon`.\n",
        " * Each grid cell can predict at most one object, but each object can be predicted by more than one grid cell.\n",
        "\n",
        "#### Target Visualization\n",
        "You should produce visualizations such as fig. 8 and 9 to validate your target assignments.\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig8.png\" width=500/></div>\n",
        "<center>Figure 8: Target assignment example.</center>\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig9.png\" width=500/></div>\n",
        "<center>Figure 9: Target assignment example.</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQfUPsRM-jhy"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "class SOLO(pl.LightningModule):\n",
        "    _default_cfg = {\n",
        "        'num_classes': 4,\n",
        "        'in_channels': 256,\n",
        "        'seg_feat_channels': 256,\n",
        "        'stacked_convs': 7,\n",
        "        'strides': [8, 8, 16, 32, 32],\n",
        "        'scale_ranges': [(1, 96), (48, 192), (96, 384), (192, 768), (384, 2048)],\n",
        "        'epsilon': 0.2,\n",
        "        'num_grids': [40, 36, 24, 16, 12],\n",
        "        'mask_loss_cfg': dict(weight=3),\n",
        "        'cate_loss_cfg': dict(gamma=2, alpha=0.25, weight=1),\n",
        "        'postprocess_cfg': dict(cate_thresh=0.2, mask_thresh=0.5, pre_NMS_num=50, keep_instance=5, IoU_thresh=0.5)\n",
        "    }\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        for k, v in {**self._default_cfg, **kwargs}.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "        pretrained_model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=True)\n",
        "        self.backbone = pretrained_model.backbone\n",
        "        ...\n",
        "\n",
        "    # Forward function should calculate across each level of the feature pyramid network.\n",
        "    # Input:\n",
        "    #     images: batch_size number of images\n",
        "    # Output:\n",
        "    #     if eval = False\n",
        "    #         category_predictions: list, len(fpn_levels), each (batch_size, C-1, S, S)\n",
        "    #         mask_predictions:     list, len(fpn_levels), each (batch_size, S^2, 2*feature_h, 2*feature_w)\n",
        "    #     if eval==True\n",
        "    #         category_predictions: list, len(fpn_levels), each (batch_size, S, S, C-1)\n",
        "    #         / after point_NMS\n",
        "    #         mask_predictions:     list, len(fpn_levels), each (batch_size, S^2, image_h/4, image_w/4)\n",
        "    #         / after upsampling\n",
        "    def forward(self, images, eval=True):\n",
        "        # you can modify this if you want to train the backbone\n",
        "        feature_pyramid = [v.detach() for v in self.backbone(images).values()] # this has strides [4,8,16,32,64]\n",
        "        ...\n",
        "\n",
        "    # This function build the ground truth tensor for each batch in the training\n",
        "    # Input:\n",
        "    #     bounding_boxes:   list, len(batch_size), each (n_object, 4) (x1 y1 x2 y2 system)\n",
        "    #     labels:           list, len(batch_size), each (n_object, )\n",
        "    #     masks:            list, len(batch_size), each (n_object, 800, 1088)\n",
        "    # Output:\n",
        "    #     category_targets: list, len(batch_size), list, len(fpn), (S, S), values are {1, 2, 3}\n",
        "    #     mask_targets:     list, len(batch_size), list, len(fpn), (S^2, 2*feature_h, 2*feature_w)\n",
        "    #     active_masks:     list, len(batch_size), list, len(fpn), (S^2,)\n",
        "    #     / boolean array with positive mask predictions\n",
        "    def generate_targets(self, bounding_boxes, labels, masks):\n",
        "        ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PwUGRtM-jh1"
      },
      "source": [
        "### Part A Submission\n",
        "In addition to the code you used, you should submit a pdf containing the following plots:\n",
        "* Dataset visualization plots such as fig. 5 and 6. You should include at least five examples and at least two examples that contain multiple objects. Additionally, the images should cover a range of scales and object classes.\n",
        "* Target assignment plots such as fig. 8 and 9. Should include at least five examples and at least two examples that contain multiple objects. Additionally, the images should cover a range of scales and object classes.\n",
        "\n",
        "We should be able to reproduce your results with a single function call or cell evaluation. Please specify the method to do so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvZZkrlV-jh3"
      },
      "source": [
        "## Part B\n",
        "### Loss\n",
        "The loss function consists of two parts: category loss and mask loss:\n",
        "$$L = \\lambda_\\text{cate} L_\\text{cate} + \\lambda_\\text{mask} L_\\text{mask} $$\n",
        "\n",
        "The category loss is defined as (where $\\text{FL}$ is the focal loss)\n",
        "\n",
        "$$L_\\text{cate} = \\frac{1}{S^2 C} \\sum_{S,S,C} \\text{FL}(p_t)$$\n",
        "\n",
        "$$\\text{FL}(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log{(p_t)}$$\n",
        "\n",
        "$$(\\alpha_t, p_t) = \\left\\{\\begin{array}{lr} (\\alpha, \\hat{p}) & \\text{if }y=1 \\\\ (1 - \\alpha, 1 - \\hat{p}) & \\text{otherwise}\\end{array}\\right.$$\n",
        "\n",
        "while the mask loss is defined as (where $d_\\text{mask}$ is the dice loss)\n",
        "\n",
        "$$L_\\text{mask} = \\frac{1}{N_\\text{positive}} \\sum_k \\mathbb{1}_{\\{p_{i, j} > 0\\}} d_\\text{mask}(m_k, \\hat{m}_k)$$\n",
        "\n",
        "$$d_\\text{mask}(p, q) = 1 - D(p, q)$$\n",
        "\n",
        "$$D(p, q) = \\frac{2 \\sum_{x,y}(p_{x,y} \\cdot q_{x,y})}{\\sum_{x,y}p_{x,y}^2 + \\sum_{x,y}q_{x,y}^2}$$\n",
        "\n",
        "### Post Processing\n",
        "Post processing consists of three steps: points NMS, concatenation and sorting, and matrix NMS. A summary of each of these steps is provided below. In addition, your final mask prediction should be a binary mask based on the mask thresholding parameter. Some examples of results post-inference are shown in figs. 11-14.\n",
        "\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig11.png\" width=300/></div>\n",
        "<center>Figure 11: SOLO instance segmentation example.</center>\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig12.png\" width=300/></div>\n",
        "<center>Figure 12: SOLO instance segmentation example.</center>\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig13.png\" width=300/></div>\n",
        "<center>Figure 13: SOLO instance segmentation example.</center>\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW3/fig14.png\" width=300/></div>\n",
        "<center>Figure 14: SOLO instance segmentation example.</center>\n",
        "\n",
        "#### Points NMS\n",
        "Non-max suppression for the category predictions, applied to each channel sperately. A maxpooling with a kernel size of 2, sample code is included below.\n",
        "\n",
        "#### Concatenation and Sorting\n",
        "Here, we merge the predictions across all the FPN levels into a single list of predictions, sorted by their predicted score. This involves rescaling the predicted masks appropriately.\n",
        "\n",
        "#### Matrix NMS\n",
        "Inspired by Soft-NMS, MatrixNMS suppresses mask predictions with a lower score based on their similarity to predictions with a higher score in a completely vectorized manner. Sample code is include below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m1KuTEt-jh4"
      },
      "source": [
        "# Credit to SOLO Author's code\n",
        "# This function does NMS on the heat map (category_prediction), grid-level\n",
        "# Input:\n",
        "#     heat: (batch_size, C-1, S, S)\n",
        "# Output:\n",
        "#     (batch_size, C-1, S, S)\n",
        "def points_nms(heat, kernel=2):\n",
        "    # kernel must be 2\n",
        "    hmax = F.max_pool2d(\n",
        "        heat, (kernel, kernel), stride=1, padding=1)\n",
        "    keep = (hmax[:, :, :-1, :-1] == heat).float()\n",
        "    return heat * keep\n",
        "\n",
        "# This function performs Matrix NMS\n",
        "# Input:\n",
        "#     sorted_masks: (n_active, image_h/4, image_w/4)\n",
        "#     sorted_scores: (n_active,)\n",
        "# Output:\n",
        "#     decay_scores: (n_active,)\n",
        "def MatrixNMS(sorted_masks, sorted_scores, method='gauss', gauss_sigma=0.5):\n",
        "    n = len(sorted_scores)\n",
        "    sorted_masks = sorted_masks.reshape(n, -1)\n",
        "    intersection = torch.mm(sorted_masks, sorted_masks.T)\n",
        "    areas = sorted_masks.sum(dim=1).expand(n, n)\n",
        "    union = areas + areas.T - intersection\n",
        "    ious = (intersection / union).triu(diagonal=1)\n",
        "\n",
        "    ious_cmax = ious.max(0)[0].expand(n, n).T\n",
        "    if method == 'gauss':\n",
        "        decay = torch.exp(-(ious ** 2 - ious_cmax ** 2) / gauss_sigma)\n",
        "    else:\n",
        "        decay = (1 - ious) / (1 - ious_cmax)\n",
        "    decay = decay.min(dim=0)[0]\n",
        "    return sorted_scores * decay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajIefq5F-jh6"
      },
      "source": [
        "### Training\n",
        "#### Optimizer\n",
        "The SGD optimizer with a weight decay of 1e-4 and a momentum of 0.9 is used. For a batch size of 16, an initial learning rate of 16 is used (this should be scaled appropriately for different batch sizes). You should train for 36 epochs, reducing the learning rate by a factor of 10 at epochs 27 and 33.\n",
        "\n",
        "#### Data Augemnetation\n",
        "To achieve better performance, you may wish to explore some data augmentation techniques. You should be able to achieve the necessary performance without any augmentation, however.\n",
        "\n",
        "#### Checkpointing\n",
        "Due to the long training time, we highly recommend that you set up regular checkpointing during your training in case your training gets interrupted (e.g. your colab session ending)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEgQbp7Y-jh7"
      },
      "source": [
        "### Part B Submission\n",
        "In addition to the code you used, you should submit a pdf containing the following plots:\n",
        "* Dataset visualization plots such as fig. 5 and 6. You should include at least five examples and at least two examples that contain multiple objects. Additionally, the images should cover a range of scales and object classes.\n",
        "* Target assignment plots such as fig. 8 and 9. You should include at least five examples and at least two examples that contain multiple objects. Additionally, the images should cover a range of scales and object classes.\n",
        "* Final inference results such as figs. 11-14. You should include at least five examples and at least two examples that contain multiple objects. Additionally, the images should cover a range of scales and object classes.\n",
        "* Training and validation loss curves. These should include the focal loss, dice loss, and total loss as seperate quantities.\n",
        "\n",
        "In addition, you should include a discussion of any issues you encountered during this project and any changes you made to the siggested architecture. If you made any interesting observations or thoughts about potential improvements, you should also include them here.\n",
        "\n",
        "* We should be able to reproduce your results with a single function call or cell evaluation. Please specify the method to do so. You do not, however, need to submit your trained weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZkuMU2WRhKW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}